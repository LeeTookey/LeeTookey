{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbI6h7E4+Th2JWLktyjO/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeTookey/LeeTookey/blob/main/Learning2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nOiom-efc_n"
      },
      "outputs": [],
      "source": [
        "\"\"\"Learning from examples (Chapters 18)\"\"\"\n",
        "\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "from statistics import stdev\n",
        "\n",
        "from qpsolvers import solve_qp\n",
        "\n",
        "from probabilistic_learning import NaiveBayesLearner\n",
        "from utils import *\n",
        "\n",
        "\n",
        "class DataSet:\n",
        "    \"\"\"\n",
        "    A data set for a machine learning problem. It has the following fields:\n",
        "\n",
        "    d.examples   A list of examples. Each one is a list of attribute values.\n",
        "    d.attrs      A list of integers to index into an example, so example[attr]\n",
        "                 gives a value. Normally the same as range(len(d.examples[0])).\n",
        "    d.attr_names Optional list of mnemonic names for corresponding attrs.\n",
        "    d.target     The attribute that a learning algorithm will try to predict.\n",
        "                 By default the final attribute.\n",
        "    d.inputs     The list of attrs without the target.\n",
        "    d.values     A list of lists: each sublist is the set of possible\n",
        "                 values for the corresponding attribute. If initially None,\n",
        "                 it is computed from the known examples by self.set_problem.\n",
        "                 If not None, an erroneous value raises ValueError.\n",
        "    d.distance   A function from a pair of examples to a non-negative number.\n",
        "                 Should be symmetric, etc. Defaults to mean_boolean_error\n",
        "                 since that can handle any field types.\n",
        "    d.name       Name of the data set (for output display only).\n",
        "    d.source     URL or other source where the data came from.\n",
        "    d.exclude    A list of attribute indexes to exclude from d.inputs. Elements\n",
        "                 of this list can either be integers (attrs) or attr_names.\n",
        "\n",
        "    Normally, you call the constructor and you're done; then you just\n",
        "    access fields like d.examples and d.target and d.inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, examples=None, attrs=None, attr_names=None, target=-1, inputs=None,\n",
        "                 values=None, distance=mean_boolean_error, name='', source='', exclude=()):\n",
        "        \"\"\"\n",
        "        Accepts any of DataSet's fields. Examples can also be a\n",
        "        string or file from which to parse examples using parse_csv.\n",
        "        Optional parameter: exclude, as documented in .set_problem().\n",
        "        >>> DataSet(examples='1, 2, 3')\n",
        "        <DataSet(): 1 examples, 3 attributes>\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.source = source\n",
        "        self.values = values\n",
        "        self.distance = distance\n",
        "        self.got_values_flag = bool(values)\n",
        "\n",
        "        # initialize .examples from string or list or data directory\n",
        "        if isinstance(examples, str):\n",
        "            self.examples = parse_csv(examples)\n",
        "        elif examples is None:\n",
        "            self.examples = parse_csv(open_data(name + '.csv').read())\n",
        "        else:\n",
        "            self.examples = examples\n",
        "\n",
        "        # attrs are the indices of examples, unless otherwise stated.\n",
        "        if self.examples is not None and attrs is None:\n",
        "            attrs = list(range(len(self.examples[0])))\n",
        "\n",
        "        self.attrs = attrs\n",
        "\n",
        "        # initialize .attr_names from string, list, or by default\n",
        "        if isinstance(attr_names, str):\n",
        "            self.attr_names = attr_names.split()\n",
        "        else:\n",
        "            self.attr_names = attr_names or attrs\n",
        "        self.set_problem(target, inputs=inputs, exclude=exclude)\n",
        "\n",
        "    def set_problem(self, target, inputs=None, exclude=()):\n",
        "        \"\"\"\n",
        "        Set (or change) the target and/or inputs.\n",
        "        This way, one DataSet can be used multiple ways. inputs, if specified,\n",
        "        is a list of attributes, or specify exclude as a list of attributes\n",
        "        to not use in inputs. Attributes can be -n .. n, or an attr_name.\n",
        "        Also computes the list of possible values, if that wasn't done yet.\n",
        "        \"\"\"\n",
        "        self.target = self.attr_num(target)\n",
        "        exclude = list(map(self.attr_num, exclude))\n",
        "        if inputs:\n",
        "            self.inputs = remove_all(self.target, inputs)\n",
        "        else:\n",
        "            self.inputs = [a for a in self.attrs if a != self.target and a not in exclude]\n",
        "        if not self.values:\n",
        "            self.update_values()\n",
        "        self.check_me()\n",
        "\n",
        "    def check_me(self):\n",
        "        \"\"\"Check that my fields make sense.\"\"\"\n",
        "        assert len(self.attr_names) == len(self.attrs)\n",
        "        assert self.target in self.attrs\n",
        "        assert self.target not in self.inputs\n",
        "        assert set(self.inputs).issubset(set(self.attrs))\n",
        "        if self.got_values_flag:\n",
        "            # only check if values are provided while initializing DataSet\n",
        "            list(map(self.check_example, self.examples))\n",
        "\n",
        "    def add_example(self, example):\n",
        "        \"\"\"Add an example to the list of examples, checking it first.\"\"\"\n",
        "        self.check_example(example)\n",
        "        self.examples.append(example)\n",
        "\n",
        "    def check_example(self, example):\n",
        "        \"\"\"Raise ValueError if example has any invalid values.\"\"\"\n",
        "        if self.values:\n",
        "            for a in self.attrs:\n",
        "                if example[a] not in self.values[a]:\n",
        "                    raise ValueError('Bad value {} for attribute {} in {}'\n",
        "                                     .format(example[a], self.attr_names[a], example))\n",
        "\n",
        "    def attr_num(self, attr):\n",
        "        \"\"\"Returns the number used for attr, which can be a name, or -n .. n-1.\"\"\"\n",
        "        if isinstance(attr, str):\n",
        "            return self.attr_names.index(attr)\n",
        "        elif attr < 0:\n",
        "            return len(self.attrs) + attr\n",
        "        else:\n",
        "            return attr\n",
        "\n",
        "    def update_values(self):\n",
        "        self.values = list(map(unique, zip(*self.examples)))\n",
        "\n",
        "    def sanitize(self, example):\n",
        "        \"\"\"Return a copy of example, with non-input attributes replaced by None.\"\"\"\n",
        "        return [attr_i if i in self.inputs else None for i, attr_i in enumerate(example)]\n",
        "\n",
        "    def classes_to_numbers(self, classes=None):\n",
        "        \"\"\"Converts class names to numbers.\"\"\"\n",
        "        if not classes:\n",
        "            # if classes were not given, extract them from values\n",
        "            classes = sorted(self.values[self.target])\n",
        "        for item in self.examples:\n",
        "            item[self.target] = classes.index(item[self.target])\n",
        "\n",
        "    def remove_examples(self, value=''):\n",
        "        \"\"\"Remove examples that contain given value.\"\"\"\n",
        "        self.examples = [x for x in self.examples if value not in x]\n",
        "        self.update_values()\n",
        "\n",
        "    def split_values_by_classes(self):\n",
        "        \"\"\"Split values into buckets according to their class.\"\"\"\n",
        "        buckets = defaultdict(lambda: [])\n",
        "        target_names = self.values[self.target]\n",
        "\n",
        "        for v in self.examples:\n",
        "            item = [a for a in v if a not in target_names]  # remove target from item\n",
        "            buckets[v[self.target]].append(item)  # add item to bucket of its class\n",
        "\n",
        "        return buckets\n",
        "\n",
        "    def find_means_and_deviations(self):\n",
        "        \"\"\"\n",
        "        Finds the means and standard deviations of self.dataset.\n",
        "        means     : a dictionary for each class/target. Holds a list of the means\n",
        "                    of the features for the class.\n",
        "        deviations: a dictionary for each class/target. Holds a list of the sample\n",
        "                    standard deviations of the features for the class.\n",
        "        \"\"\"\n",
        "        target_names = self.values[self.target]\n",
        "        feature_numbers = len(self.inputs)\n",
        "\n",
        "        item_buckets = self.split_values_by_classes()\n",
        "\n",
        "        means = defaultdict(lambda: [0] * feature_numbers)\n",
        "        deviations = defaultdict(lambda: [0] * feature_numbers)\n",
        "\n",
        "        for t in target_names:\n",
        "            # find all the item feature values for item in class t\n",
        "            features = [[] for _ in range(feature_numbers)]\n",
        "            for item in item_buckets[t]:\n",
        "                for i in range(feature_numbers):\n",
        "                    features[i].append(item[i])\n",
        "\n",
        "            # calculate means and deviations fo the class\n",
        "            for i in range(feature_numbers):\n",
        "                means[t][i] = mean(features[i])\n",
        "                deviations[t][i] = stdev(features[i])\n",
        "\n",
        "        return means, deviations\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '<DataSet({}): {:d} examples, {:d} attributes>'.format(self.name, len(self.examples), len(self.attrs))\n",
        "\n",
        "\n",
        "def parse_csv(input, delim=','):\n",
        "    r\"\"\"\n",
        "    Input is a string consisting of lines, each line has comma-delimited\n",
        "    fields. Convert this into a list of lists. Blank lines are skipped.\n",
        "    Fields that look like numbers are converted to numbers.\n",
        "    The delim defaults to ',' but '\\t' and None are also reasonable values.\n",
        "    >>> parse_csv('1, 2, 3 \\n 0, 2, na')\n",
        "    [[1, 2, 3], [0, 2, 'na']]\n",
        "    \"\"\"\n",
        "    lines = [line for line in input.splitlines() if line.strip()]\n",
        "    return [list(map(num_or_str, line.split(delim))) for line in lines]\n",
        "\n",
        "\n",
        "def err_ratio(predict, dataset, examples=None):\n",
        "    \"\"\"\n",
        "    Return the proportion of the examples that are NOT correctly predicted.\n",
        "    verbose - 0: No output; 1: Output wrong; 2 (or greater): Output correct\n",
        "    \"\"\"\n",
        "    examples = examples or dataset.examples\n",
        "    if len(examples) == 0:\n",
        "        return 0.0\n",
        "    right = 0\n",
        "    for example in examples:\n",
        "        desired = example[dataset.target]\n",
        "        output = predict(dataset.sanitize(example))\n",
        "        if output == desired:\n",
        "            right += 1\n",
        "    return 1 - (right / len(examples))\n",
        "\n",
        "\n",
        "def grade_learner(predict, tests):\n",
        "    \"\"\"\n",
        "    Grades the given learner based on how many tests it passes.\n",
        "    tests is a list with each element in the form: (values, output).\n",
        "    \"\"\"\n",
        "    return mean(int(predict(X) == y) for X, y in tests)\n",
        "\n",
        "\n",
        "def train_test_split(dataset, start=None, end=None, test_split=None):\n",
        "    \"\"\"\n",
        "    If you are giving 'start' and 'end' as parameters,\n",
        "    then it will return the testing set from index 'start' to 'end'\n",
        "    and the rest for training.\n",
        "    If you give 'test_split' as a parameter then it will return\n",
        "    test_split * 100% as the testing set and the rest as\n",
        "    training set.\n",
        "    \"\"\"\n",
        "    examples = dataset.examples\n",
        "    if test_split is None:\n",
        "        train = examples[:start] + examples[end:]\n",
        "        val = examples[start:end]\n",
        "    else:\n",
        "        total_size = len(examples)\n",
        "        val_size = int(total_size * test_split)\n",
        "        train_size = total_size - val_size\n",
        "        train = examples[:train_size]\n",
        "        val = examples[train_size:total_size]\n",
        "\n",
        "    return train, val\n",
        "\n",
        "\n",
        "def cross_validation_wrapper(learner, dataset, k=10, trials=1):\n",
        "    \"\"\"\n",
        "    [Figure 18.8]\n",
        "    Return the optimal value of size having minimum error on validation set.\n",
        "    errT: a training error array, indexed by size\n",
        "    errV: a validation error array, indexed by size\n",
        "    \"\"\"\n",
        "    errs = []\n",
        "    size = 1\n",
        "    while True:\n",
        "        errT, errV = cross_validation(learner, dataset, size, k, trials)\n",
        "        # check for convergence provided err_val is not empty\n",
        "        if errT and not np.isclose(errT[-1], errT, rtol=1e-6):\n",
        "            best_size = 0\n",
        "            min_val = np.inf\n",
        "            i = 0\n",
        "            while i < size:\n",
        "                if errs[i] < min_val:\n",
        "                    min_val = errs[i]\n",
        "                    best_size = i\n",
        "                i += 1\n",
        "            return learner(dataset, best_size)\n",
        "        errs.append(errV)\n",
        "        size += 1\n",
        "\n",
        "\n",
        "def cross_validation(learner, dataset, size=None, k=10, trials=1):\n",
        "    \"\"\"\n",
        "    Do k-fold cross_validate and return their mean.\n",
        "    That is, keep out 1/k of the examples for testing on each of k runs.\n",
        "    Shuffle the examples first; if trials > 1, average over several shuffles.\n",
        "    Returns Training error, Validation error\n",
        "    \"\"\"\n",
        "    k = k or len(dataset.examples)\n",
        "    if trials > 1:\n",
        "        trial_errT = 0\n",
        "        trial_errV = 0\n",
        "        for t in range(trials):\n",
        "            errT, errV = cross_validation(learner, dataset, size, k, trials)\n",
        "            trial_errT += errT\n",
        "            trial_errV += errV\n",
        "        return trial_errT / trials, trial_errV / trials\n",
        "    else:\n",
        "        fold_errT = 0\n",
        "        fold_errV = 0\n",
        "        n = len(dataset.examples)\n",
        "        examples = dataset.examples\n",
        "        random.shuffle(dataset.examples)\n",
        "        for fold in range(k):\n",
        "            train_data, val_data = train_test_split(dataset, fold * (n // k), (fold + 1) * (n // k))\n",
        "            dataset.examples = train_data\n",
        "            h = learner(dataset, size)\n",
        "            fold_errT += err_ratio(h, dataset, train_data)\n",
        "            fold_errV += err_ratio(h, dataset, val_data)\n",
        "            # reverting back to original once test is completed\n",
        "            dataset.examples = examples\n",
        "        return fold_errT / k, fold_errV / k\n",
        "\n",
        "\n",
        "def leave_one_out(learner, dataset, size=None):\n",
        "    \"\"\"Leave one out cross-validation over the dataset.\"\"\"\n",
        "    return cross_validation(learner, dataset, size, len(dataset.examples))\n",
        "\n",
        "\n",
        "def learning_curve(learner, dataset, trials=10, sizes=None):\n",
        "    if sizes is None:\n",
        "        sizes = list(range(2, len(dataset.examples) - trials, 2))\n",
        "\n",
        "    def score(learner, size):\n",
        "        random.shuffle(dataset.examples)\n",
        "        return cross_validation(learner, dataset, size, trials)\n",
        "\n",
        "    return [(size, mean([score(learner, size) for _ in range(trials)])) for size in sizes]\n",
        "\n",
        "\n",
        "def PluralityLearner(dataset):\n",
        "    \"\"\"\n",
        "    A very dumb algorithm: always pick the result that was most popular\n",
        "    in the training data. Makes a baseline for comparison.\n",
        "    \"\"\"\n",
        "    most_popular = mode([e[dataset.target] for e in dataset.examples])\n",
        "\n",
        "    def predict(example):\n",
        "        \"\"\"Always return same result: the most popular from the training set.\"\"\"\n",
        "        return most_popular\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "class DecisionFork:\n",
        "    \"\"\"\n",
        "    A fork of a decision tree holds an attribute to test, and a dict\n",
        "    of branches, one for each of the attribute's values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attr, attr_name=None, default_child=None, branches=None):\n",
        "        \"\"\"Initialize by saying what attribute this node tests.\"\"\"\n",
        "        self.attr = attr\n",
        "        self.attr_name = attr_name or attr\n",
        "        self.default_child = default_child\n",
        "        self.branches = branches or {}\n",
        "\n",
        "    def __call__(self, example):\n",
        "        \"\"\"Given an example, classify it using the attribute and the branches.\"\"\"\n",
        "        attr_val = example[self.attr]\n",
        "        if attr_val in self.branches:\n",
        "            return self.branches[attr_val](example)\n",
        "        else:\n",
        "            # return default class when attribute is unknown\n",
        "            return self.default_child(example)\n",
        "\n",
        "    def add(self, val, subtree):\n",
        "        \"\"\"Add a branch. If self.attr = val, go to the given subtree.\"\"\"\n",
        "        self.branches[val] = subtree\n",
        "\n",
        "    def display(self, indent=0):\n",
        "        name = self.attr_name\n",
        "        print('Test', name)\n",
        "        for (val, subtree) in self.branches.items():\n",
        "            print(' ' * 4 * indent, name, '=', val, '==>', end=' ')\n",
        "            subtree.display(indent + 1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'DecisionFork({0!r}, {1!r}, {2!r})'.format(self.attr, self.attr_name, self.branches)\n",
        "\n",
        "\n",
        "class DecisionLeaf:\n",
        "    \"\"\"A leaf of a decision tree holds just a result.\"\"\"\n",
        "\n",
        "    def __init__(self, result):\n",
        "        self.result = result\n",
        "\n",
        "    def __call__(self, example):\n",
        "        return self.result\n",
        "\n",
        "    def display(self):\n",
        "        print('RESULT =', self.result)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return repr(self.result)\n",
        "\n",
        "\n",
        "def DecisionTreeLearner(dataset):\n",
        "    \"\"\"[Figure 18.5]\"\"\"\n",
        "\n",
        "    target, values = dataset.target, dataset.values\n",
        "\n",
        "    def decision_tree_learning(examples, attrs, parent_examples=()):\n",
        "        if len(examples) == 0:\n",
        "            return plurality_value(parent_examples)\n",
        "        if all_same_class(examples):\n",
        "            return DecisionLeaf(examples[0][target])\n",
        "        if len(attrs) == 0:\n",
        "            return plurality_value(examples)\n",
        "        A = choose_attribute(attrs, examples)\n",
        "        tree = DecisionFork(A, dataset.attr_names[A], plurality_value(examples))\n",
        "        for (v_k, exs) in split_by(A, examples):\n",
        "            subtree = decision_tree_learning(exs, remove_all(A, attrs), examples)\n",
        "            tree.add(v_k, subtree)\n",
        "        return tree\n",
        "\n",
        "    def plurality_value(examples):\n",
        "        \"\"\"\n",
        "        Return the most popular target value for this set of examples.\n",
        "        (If target is binary, this is the majority; otherwise plurality).\n",
        "        \"\"\"\n",
        "        popular = argmax_random_tie(values[target], key=lambda v: count(target, v, examples))\n",
        "        return DecisionLeaf(popular)\n",
        "\n",
        "    def count(attr, val, examples):\n",
        "        \"\"\"Count the number of examples that have example[attr] = val.\"\"\"\n",
        "        return sum(e[attr] == val for e in examples)\n",
        "\n",
        "    def all_same_class(examples):\n",
        "        \"\"\"Are all these examples in the same target class?\"\"\"\n",
        "        class0 = examples[0][target]\n",
        "        return all(e[target] == class0 for e in examples)\n",
        "\n",
        "    def choose_attribute(attrs, examples):\n",
        "        \"\"\"Choose the attribute with the highest information gain.\"\"\"\n",
        "        return argmax_random_tie(attrs, key=lambda a: information_gain(a, examples))\n",
        "\n",
        "    def information_gain(attr, examples):\n",
        "        \"\"\"Return the expected reduction in entropy from splitting by attr.\"\"\"\n",
        "\n",
        "        def I(examples):\n",
        "            return information_content([count(target, v, examples) for v in values[target]])\n",
        "\n",
        "        n = len(examples)\n",
        "        remainder = sum((len(examples_i) / n) * I(examples_i) for (v, examples_i) in split_by(attr, examples))\n",
        "        return I(examples) - remainder\n",
        "\n",
        "    def split_by(attr, examples):\n",
        "        \"\"\"Return a list of (val, examples) pairs for each val of attr.\"\"\"\n",
        "        return [(v, [e for e in examples if e[attr] == v]) for v in values[attr]]\n",
        "\n",
        "    return decision_tree_learning(dataset.examples, dataset.inputs)\n",
        "\n",
        "\n",
        "def information_content(values):\n",
        "    \"\"\"Number of bits to represent the probability distribution in values.\"\"\"\n",
        "    probabilities = normalize(remove_all(0, values))\n",
        "    return sum(-p * np.log2(p) for p in probabilities)\n",
        "\n",
        "\n",
        "def DecisionListLearner(dataset):\n",
        "    \"\"\"\n",
        "    [Figure 18.11]\n",
        "    A decision list implemented as a list of (test, value) pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def decision_list_learning(examples):\n",
        "        if not examples:\n",
        "            return [(True, False)]\n",
        "        t, o, examples_t = find_examples(examples)\n",
        "        if not t:\n",
        "            raise Exception\n",
        "        return [(t, o)] + decision_list_learning(examples - examples_t)\n",
        "\n",
        "    def find_examples(examples):\n",
        "        \"\"\"\n",
        "        Find a set of examples that all have the same outcome under\n",
        "        some test. Return a tuple of the test, outcome, and examples.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def passes(example, test):\n",
        "        \"\"\"Does the example pass the test?\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(example):\n",
        "        \"\"\"Predict the outcome for the first passing test.\"\"\"\n",
        "        for test, outcome in predict.decision_list:\n",
        "            if passes(example, test):\n",
        "                return outcome\n",
        "\n",
        "    predict.decision_list = decision_list_learning(set(dataset.examples))\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def NearestNeighborLearner(dataset, k=1):\n",
        "    \"\"\"k-NearestNeighbor: the k nearest neighbors vote.\"\"\"\n",
        "\n",
        "    def predict(example):\n",
        "        \"\"\"Find the k closest items, and have them vote for the best.\"\"\"\n",
        "        best = heapq.nsmallest(k, ((dataset.distance(e, example), e) for e in dataset.examples))\n",
        "        return mode(e[dataset.target] for (d, e) in best)\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def LinearLearner(dataset, learning_rate=0.01, epochs=100):\n",
        "    \"\"\"\n",
        "    [Section 18.6.3]\n",
        "    Linear classifier with hard threshold.\n",
        "    \"\"\"\n",
        "    idx_i = dataset.inputs\n",
        "    idx_t = dataset.target\n",
        "    examples = dataset.examples\n",
        "    num_examples = len(examples)\n",
        "\n",
        "    # X transpose\n",
        "    X_col = [dataset.values[i] for i in idx_i]  # vertical columns of X\n",
        "\n",
        "    # add dummy\n",
        "    ones = [1 for _ in range(len(examples))]\n",
        "    X_col = [ones] + X_col\n",
        "\n",
        "    # initialize random weights\n",
        "    num_weights = len(idx_i) + 1\n",
        "    w = random_weights(min_value=-0.5, max_value=0.5, num_weights=num_weights)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        err = []\n",
        "        # pass over all examples\n",
        "        for example in examples:\n",
        "            x = [1] + example\n",
        "            y = np.dot(w, x)\n",
        "            t = example[idx_t]\n",
        "            err.append(t - y)\n",
        "\n",
        "        # update weights\n",
        "        for i in range(len(w)):\n",
        "            w[i] = w[i] + learning_rate * (np.dot(err, X_col[i]) / num_examples)\n",
        "\n",
        "    def predict(example):\n",
        "        x = [1] + example\n",
        "        return np.dot(w, x)\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def LogisticLinearLeaner(dataset, learning_rate=0.01, epochs=100):\n",
        "    \"\"\"\n",
        "    [Section 18.6.4]\n",
        "    Linear classifier with logistic regression.\n",
        "    \"\"\"\n",
        "    idx_i = dataset.inputs\n",
        "    idx_t = dataset.target\n",
        "    examples = dataset.examples\n",
        "    num_examples = len(examples)\n",
        "\n",
        "    # X transpose\n",
        "    X_col = [dataset.values[i] for i in idx_i]  # vertical columns of X\n",
        "\n",
        "    # add dummy\n",
        "    ones = [1 for _ in range(len(examples))]\n",
        "    X_col = [ones] + X_col\n",
        "\n",
        "    # initialize random weights\n",
        "    num_weights = len(idx_i) + 1\n",
        "    w = random_weights(min_value=-0.5, max_value=0.5, num_weights=num_weights)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        err = []\n",
        "        h = []\n",
        "        # pass over all examples\n",
        "        for example in examples:\n",
        "            x = [1] + example\n",
        "            y = sigmoid(np.dot(w, x))\n",
        "            h.append(sigmoid_derivative(y))\n",
        "            t = example[idx_t]\n",
        "            err.append(t - y)\n",
        "\n",
        "        # update weights\n",
        "        for i in range(len(w)):\n",
        "            buffer = [x * y for x, y in zip(err, h)]\n",
        "            w[i] = w[i] + learning_rate * (np.dot(buffer, X_col[i]) / num_examples)\n",
        "\n",
        "    def predict(example):\n",
        "        x = [1] + example\n",
        "        return sigmoid(np.dot(w, x))\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def NeuralNetLearner(dataset, hidden_layer_sizes=None, learning_rate=0.01, epochs=100, activation=sigmoid):\n",
        "    \"\"\"\n",
        "    Layered feed-forward network.\n",
        "    hidden_layer_sizes: List of number of hidden units per hidden layer\n",
        "    learning_rate: Learning rate of gradient descent\n",
        "    epochs: Number of passes over the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    if hidden_layer_sizes is None:\n",
        "        hidden_layer_sizes = [3]\n",
        "    i_units = len(dataset.inputs)\n",
        "    o_units = len(dataset.values[dataset.target])\n",
        "\n",
        "    # construct a network\n",
        "    raw_net = network(i_units, hidden_layer_sizes, o_units, activation)\n",
        "    learned_net = BackPropagationLearner(dataset, raw_net, learning_rate, epochs, activation)\n",
        "\n",
        "    def predict(example):\n",
        "        # input nodes\n",
        "        i_nodes = learned_net[0]\n",
        "\n",
        "        # activate input layer\n",
        "        for v, n in zip(example, i_nodes):\n",
        "            n.value = v\n",
        "\n",
        "        # forward pass\n",
        "        for layer in learned_net[1:]:\n",
        "            for node in layer:\n",
        "                inc = [n.value for n in node.inputs]\n",
        "                in_val = dot_product(inc, node.weights)\n",
        "                node.value = node.activation(in_val)\n",
        "\n",
        "        # hypothesis\n",
        "        o_nodes = learned_net[-1]\n",
        "        prediction = find_max_node(o_nodes)\n",
        "        return prediction\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def BackPropagationLearner(dataset, net, learning_rate, epochs, activation=sigmoid):\n",
        "    \"\"\"\n",
        "    [Figure 18.23]\n",
        "    The back-propagation algorithm for multilayer networks.\n",
        "    \"\"\"\n",
        "    # initialise weights\n",
        "    for layer in net:\n",
        "        for node in layer:\n",
        "            node.weights = random_weights(min_value=-0.5, max_value=0.5, num_weights=len(node.weights))\n",
        "\n",
        "    examples = dataset.examples\n",
        "    # As of now dataset.target gives an int instead of list,\n",
        "    # Changing dataset class will have effect on all the learners.\n",
        "    # Will be taken care of later.\n",
        "    o_nodes = net[-1]\n",
        "    i_nodes = net[0]\n",
        "    o_units = len(o_nodes)\n",
        "    idx_t = dataset.target\n",
        "    idx_i = dataset.inputs\n",
        "    n_layers = len(net)\n",
        "\n",
        "    inputs, targets = init_examples(examples, idx_i, idx_t, o_units)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # iterate over each example\n",
        "        for e in range(len(examples)):\n",
        "            i_val = inputs[e]\n",
        "            t_val = targets[e]\n",
        "\n",
        "            # activate input layer\n",
        "            for v, n in zip(i_val, i_nodes):\n",
        "                n.value = v\n",
        "\n",
        "            # forward pass\n",
        "            for layer in net[1:]:\n",
        "                for node in layer:\n",
        "                    inc = [n.value for n in node.inputs]\n",
        "                    in_val = dot_product(inc, node.weights)\n",
        "                    node.value = node.activation(in_val)\n",
        "\n",
        "            # initialize delta\n",
        "            delta = [[] for _ in range(n_layers)]\n",
        "\n",
        "            # compute outer layer delta\n",
        "\n",
        "            # error for the MSE cost function\n",
        "            err = [t_val[i] - o_nodes[i].value for i in range(o_units)]\n",
        "\n",
        "            # calculate delta at output\n",
        "            if node.activation == sigmoid:\n",
        "                delta[-1] = [sigmoid_derivative(o_nodes[i].value) * err[i] for i in range(o_units)]\n",
        "            elif node.activation == relu:\n",
        "                delta[-1] = [relu_derivative(o_nodes[i].value) * err[i] for i in range(o_units)]\n",
        "            elif node.activation == tanh:\n",
        "                delta[-1] = [tanh_derivative(o_nodes[i].value) * err[i] for i in range(o_units)]\n",
        "            elif node.activation == elu:\n",
        "                delta[-1] = [elu_derivative(o_nodes[i].value) * err[i] for i in range(o_units)]\n",
        "            elif node.activation == leaky_relu:\n",
        "                delta[-1] = [leaky_relu_derivative(o_nodes[i].value) * err[i] for i in range(o_units)]\n",
        "            else:\n",
        "                return ValueError(\"Activation function unknown.\")\n",
        "\n",
        "            # backward pass\n",
        "            h_layers = n_layers - 2\n",
        "            for i in range(h_layers, 0, -1):\n",
        "                layer = net[i]\n",
        "                h_units = len(layer)\n",
        "                nx_layer = net[i + 1]\n",
        "\n",
        "                # weights from each ith layer node to each i + 1th layer node\n",
        "                w = [[node.weights[k] for node in nx_layer] for k in range(h_units)]\n",
        "\n",
        "                if activation == sigmoid:\n",
        "                    delta[i] = [sigmoid_derivative(layer[j].value) * dot_product(w[j], delta[i + 1])\n",
        "                                for j in range(h_units)]\n",
        "                elif activation == relu:\n",
        "                    delta[i] = [relu_derivative(layer[j].value) * dot_product(w[j], delta[i + 1])\n",
        "                                for j in range(h_units)]\n",
        "                elif activation == tanh:\n",
        "                    delta[i] = [tanh_derivative(layer[j].value) * dot_product(w[j], delta[i + 1])\n",
        "                                for j in range(h_units)]\n",
        "                elif activation == elu:\n",
        "                    delta[i] = [elu_derivative(layer[j].value) * dot_product(w[j], delta[i + 1])\n",
        "                                for j in range(h_units)]\n",
        "                elif activation == leaky_relu:\n",
        "                    delta[i] = [leaky_relu_derivative(layer[j].value) * dot_product(w[j], delta[i + 1])\n",
        "                                for j in range(h_units)]\n",
        "                else:\n",
        "                    return ValueError(\"Activation function unknown.\")\n",
        "\n",
        "            # update weights\n",
        "            for i in range(1, n_layers):\n",
        "                layer = net[i]\n",
        "                inc = [node.value for node in net[i - 1]]\n",
        "                units = len(layer)\n",
        "                for j in range(units):\n",
        "                    layer[j].weights = vector_add(layer[j].weights,\n",
        "                                                  scalar_vector_product(learning_rate * delta[i][j], inc))\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "def PerceptronLearner(dataset, learning_rate=0.01, epochs=100):\n",
        "    \"\"\"Logistic Regression, NO hidden layer\"\"\"\n",
        "    i_units = len(dataset.inputs)\n",
        "    o_units = len(dataset.values[dataset.target])\n",
        "    hidden_layer_sizes = []\n",
        "    raw_net = network(i_units, hidden_layer_sizes, o_units)\n",
        "    learned_net = BackPropagationLearner(dataset, raw_net, learning_rate, epochs)\n",
        "\n",
        "    def predict(example):\n",
        "        o_nodes = learned_net[1]\n",
        "\n",
        "        # forward pass\n",
        "        for node in o_nodes:\n",
        "            in_val = dot_product(example, node.weights)\n",
        "            node.value = node.activation(in_val)\n",
        "\n",
        "        # hypothesis\n",
        "        return find_max_node(o_nodes)\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "class NNUnit:\n",
        "    \"\"\"\n",
        "    Single Unit of Multiple Layer Neural Network\n",
        "    inputs: Incoming connections\n",
        "    weights: Weights to incoming connections\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, activation=sigmoid, weights=None, inputs=None):\n",
        "        self.weights = weights or []\n",
        "        self.inputs = inputs or []\n",
        "        self.value = None\n",
        "        self.activation = activation\n",
        "\n",
        "\n",
        "def network(input_units, hidden_layer_sizes, output_units, activation=sigmoid):\n",
        "    \"\"\"\n",
        "    Create Directed Acyclic Network of given number layers.\n",
        "    hidden_layers_sizes : List number of neuron units in each hidden layer\n",
        "    excluding input and output layers\n",
        "    \"\"\"\n",
        "    layers_sizes = [input_units] + hidden_layer_sizes + [output_units]\n",
        "\n",
        "    net = [[NNUnit(activation) for _ in range(size)] for size in layers_sizes]\n",
        "    n_layers = len(net)\n",
        "\n",
        "    # make connection\n",
        "    for i in range(1, n_layers):\n",
        "        for n in net[i]:\n",
        "            for k in net[i - 1]:\n",
        "                n.inputs.append(k)\n",
        "                n.weights.append(0)\n",
        "    return net\n",
        "\n",
        "\n",
        "def init_examples(examples, idx_i, idx_t, o_units):\n",
        "    inputs, targets = {}, {}\n",
        "\n",
        "    for i, e in enumerate(examples):\n",
        "        # input values of e\n",
        "        inputs[i] = [e[i] for i in idx_i]\n",
        "\n",
        "        if o_units > 1:\n",
        "            # one-hot representation of e's target\n",
        "            t = [0 for i in range(o_units)]\n",
        "            t[e[idx_t]] = 1\n",
        "            targets[i] = t\n",
        "        else:\n",
        "            # target value of e\n",
        "            targets[i] = [e[idx_t]]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "def find_max_node(nodes):\n",
        "    return nodes.index(max(nodes, key=lambda node: node.value))\n",
        "\n",
        "\n",
        "class SVC:\n",
        "\n",
        "    def __init__(self, kernel=linear_kernel, C=1.0, verbose=False):\n",
        "        self.kernel = kernel\n",
        "        self.C = C  # hyper-parameter\n",
        "        self.sv_idx, self.sv, self.sv_y = np.zeros(0), np.zeros(0), np.zeros(0)\n",
        "        self.alphas = np.zeros(0)\n",
        "        self.w = None\n",
        "        self.b = 0.0  # intercept\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the model by solving a quadratic programming problem.\n",
        "        :param X: array of size [n_samples, n_features] holding the training samples\n",
        "        :param y: array of size [n_samples] holding the class labels\n",
        "        \"\"\"\n",
        "        # In QP formulation (dual): m variables, 2m+1 constraints (1 equation, 2m inequations)\n",
        "        self.solve_qp(X, y)\n",
        "        sv = self.alphas > 1e-5\n",
        "        self.sv_idx = np.arange(len(self.alphas))[sv]\n",
        "        self.sv, self.sv_y, self.alphas = X[sv], y[sv], self.alphas[sv]\n",
        "\n",
        "        if self.kernel == linear_kernel:\n",
        "            self.w = np.dot(self.alphas * self.sv_y, self.sv)\n",
        "\n",
        "        for n in range(len(self.alphas)):\n",
        "            self.b += self.sv_y[n]\n",
        "            self.b -= np.sum(self.alphas * self.sv_y * self.K[self.sv_idx[n], sv])\n",
        "        self.b /= len(self.alphas)\n",
        "        return self\n",
        "\n",
        "    def solve_qp(self, X, y):\n",
        "        \"\"\"\n",
        "        Solves a quadratic programming problem. In QP formulation (dual):\n",
        "        m variables, 2m+1 constraints (1 equation, 2m inequations).\n",
        "        :param X: array of size [n_samples, n_features] holding the training samples\n",
        "        :param y: array of size [n_samples] holding the class labels\n",
        "        \"\"\"\n",
        "        m = len(y)  # m = n_samples\n",
        "        self.K = self.kernel(X)  # gram matrix\n",
        "        P = self.K * np.outer(y, y)\n",
        "        q = -np.ones(m)\n",
        "        lb = np.zeros(m)  # lower bounds\n",
        "        ub = np.ones(m) * self.C  # upper bounds\n",
        "        A = y.astype(np.float64)  # equality matrix\n",
        "        b = np.zeros(1)  # equality vector\n",
        "        self.alphas = solve_qp(P, q, A=A, b=b, lb=lb, ub=ub, solver='cvxopt',\n",
        "                               sym_proj=True, verbose=self.verbose)\n",
        "\n",
        "    def predict_score(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the score for a given example.\n",
        "        \"\"\"\n",
        "        if self.w is None:\n",
        "            return np.dot(self.alphas * self.sv_y, self.kernel(self.sv, X)) + self.b\n",
        "        return np.dot(X, self.w) + self.b\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the class of a given example.\n",
        "        \"\"\"\n",
        "        return np.sign(self.predict_score(X))\n",
        "\n",
        "\n",
        "class SVR:\n",
        "\n",
        "    def __init__(self, kernel=linear_kernel, C=1.0, epsilon=0.1, verbose=False):\n",
        "        self.kernel = kernel\n",
        "        self.C = C  # hyper-parameter\n",
        "        self.epsilon = epsilon  # epsilon insensitive loss value\n",
        "        self.sv_idx, self.sv = np.zeros(0), np.zeros(0)\n",
        "        self.alphas_p, self.alphas_n = np.zeros(0), np.zeros(0)\n",
        "        self.w = None\n",
        "        self.b = 0.0  # intercept\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the model by solving a quadratic programming problem.\n",
        "        :param X: array of size [n_samples, n_features] holding the training samples\n",
        "        :param y: array of size [n_samples] holding the class labels\n",
        "        \"\"\"\n",
        "        # In QP formulation (dual): m variables, 2m+1 constraints (1 equation, 2m inequations)\n",
        "        self.solve_qp(X, y)\n",
        "\n",
        "        sv = np.logical_or(self.alphas_p > 1e-5, self.alphas_n > 1e-5)\n",
        "        self.sv_idx = np.arange(len(self.alphas_p))[sv]\n",
        "        self.sv, sv_y = X[sv], y[sv]\n",
        "        self.alphas_p, self.alphas_n = self.alphas_p[sv], self.alphas_n[sv]\n",
        "\n",
        "        if self.kernel == linear_kernel:\n",
        "            self.w = np.dot(self.alphas_p - self.alphas_n, self.sv)\n",
        "\n",
        "        for n in range(len(self.alphas_p)):\n",
        "            self.b += sv_y[n]\n",
        "            self.b -= np.sum((self.alphas_p - self.alphas_n) * self.K[self.sv_idx[n], sv])\n",
        "        self.b -= self.epsilon\n",
        "        self.b /= len(self.alphas_p)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def solve_qp(self, X, y):\n",
        "        \"\"\"\n",
        "        Solves a quadratic programming problem. In QP formulation (dual):\n",
        "        m variables, 2m+1 constraints (1 equation, 2m inequations).\n",
        "        :param X: array of size [n_samples, n_features] holding the training samples\n",
        "        :param y: array of size [n_samples] holding the class labels\n",
        "        \"\"\"\n",
        "        #\n",
        "        m = len(y)  # m = n_samples\n",
        "        self.K = self.kernel(X)  # gram matrix\n",
        "        P = np.vstack((np.hstack((self.K, -self.K)),  # alphas_p, alphas_n\n",
        "                       np.hstack((-self.K, self.K))))  # alphas_n, alphas_p\n",
        "        q = np.hstack((-y, y)) + self.epsilon\n",
        "        lb = np.zeros(2 * m)  # lower bounds\n",
        "        ub = np.ones(2 * m) * self.C  # upper bounds\n",
        "        A = np.hstack((np.ones(m), -np.ones(m)))  # equality matrix\n",
        "        b = np.zeros(1)  # equality vector\n",
        "        alphas = solve_qp(P, q, A=A, b=b, lb=lb, ub=ub, solver='cvxopt',\n",
        "                          sym_proj=True, verbose=self.verbose)\n",
        "        self.alphas_p = alphas[:m]\n",
        "        self.alphas_n = alphas[m:]\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.kernel != linear_kernel:\n",
        "            return np.dot(self.alphas_p - self.alphas_n, self.kernel(self.sv, X)) + self.b\n",
        "        return np.dot(X, self.w) + self.b\n",
        "\n",
        "\n",
        "class MultiClassLearner:\n",
        "\n",
        "    def __init__(self, clf, decision_function='ovr'):\n",
        "        self.clf = clf\n",
        "        self.decision_function = decision_function\n",
        "        self.n_class, self.classifiers = 0, []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains n_class or n_class * (n_class - 1) / 2 classifiers\n",
        "        according to the training method, ovr or ovo respectively.\n",
        "        :param X: array of size [n_samples, n_features] holding the training samples\n",
        "        :param y: array of size [n_samples] holding the class labels\n",
        "        :return: array of classifiers\n",
        "        \"\"\"\n",
        "        labels = np.unique(y)\n",
        "        self.n_class = len(labels)\n",
        "        if self.decision_function == 'ovr':  # one-vs-rest method\n",
        "            for label in labels:\n",
        "                y1 = np.array(y)\n",
        "                y1[y1 != label] = -1.0\n",
        "                y1[y1 == label] = 1.0\n",
        "                self.clf.fit(X, y1)\n",
        "                self.classifiers.append(copy.deepcopy(self.clf))\n",
        "        elif self.decision_function == 'ovo':  # use one-vs-one method\n",
        "            n_labels = len(labels)\n",
        "            for i in range(n_labels):\n",
        "                for j in range(i + 1, n_labels):\n",
        "                    neg_id, pos_id = y == labels[i], y == labels[j]\n",
        "                    X1, y1 = np.r_[X[neg_id], X[pos_id]], np.r_[y[neg_id], y[pos_id]]\n",
        "                    y1[y1 == labels[i]] = -1.0\n",
        "                    y1[y1 == labels[j]] = 1.0\n",
        "                    self.clf.fit(X1, y1)\n",
        "                    self.classifiers.append(copy.deepcopy(self.clf))\n",
        "        else:\n",
        "            return ValueError(\"Decision function must be either 'ovr' or 'ovo'.\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the class of a given example according to the training method.\n",
        "        \"\"\"\n",
        "        n_samples = len(X)\n",
        "        if self.decision_function == 'ovr':  # one-vs-rest method\n",
        "            assert len(self.classifiers) == self.n_class\n",
        "            score = np.zeros((n_samples, self.n_class))\n",
        "            for i in range(self.n_class):\n",
        "                clf = self.classifiers[i]\n",
        "                score[:, i] = clf.predict_score(X)\n",
        "            return np.argmax(score, axis=1)\n",
        "        elif self.decision_function == 'ovo':  # use one-vs-one method\n",
        "            assert len(self.classifiers) == self.n_class * (self.n_class - 1) / 2\n",
        "            vote = np.zeros((n_samples, self.n_class))\n",
        "            clf_id = 0\n",
        "            for i in range(self.n_class):\n",
        "                for j in range(i + 1, self.n_class):\n",
        "                    res = self.classifiers[clf_id].predict(X)\n",
        "                    vote[res < 0, i] += 1.0  # negative sample: class i\n",
        "                    vote[res > 0, j] += 1.0  # positive sample: class j\n",
        "                    clf_id += 1\n",
        "            return np.argmax(vote, axis=1)\n",
        "        else:\n",
        "            return ValueError(\"Decision function must be either 'ovr' or 'ovo'.\")\n",
        "\n",
        "\n",
        "def EnsembleLearner(learners):\n",
        "    \"\"\"Given a list of learning algorithms, have them vote.\"\"\"\n",
        "\n",
        "    def train(dataset):\n",
        "        predictors = [learner(dataset) for learner in learners]\n",
        "\n",
        "        def predict(example):\n",
        "            return mode(predictor(example) for predictor in predictors)\n",
        "\n",
        "        return predict\n",
        "\n",
        "    return train\n",
        "\n",
        "\n",
        "def ada_boost(dataset, L, K):\n",
        "    \"\"\"[Figure 18.34]\"\"\"\n",
        "\n",
        "    examples, target = dataset.examples, dataset.target\n",
        "    n = len(examples)\n",
        "    eps = 1 / (2 * n)\n",
        "    w = [1 / n] * n\n",
        "    h, z = [], []\n",
        "    for k in range(K):\n",
        "        h_k = L(dataset, w)\n",
        "        h.append(h_k)\n",
        "        error = sum(weight for example, weight in zip(examples, w) if example[target] != h_k(example))\n",
        "        # avoid divide-by-0 from either 0% or 100% error rates\n",
        "        error = np.clip(error, eps, 1 - eps)\n",
        "        for j, example in enumerate(examples):\n",
        "            if example[target] == h_k(example):\n",
        "                w[j] *= error / (1 - error)\n",
        "        w = normalize(w)\n",
        "        z.append(np.log((1 - error) / error))\n",
        "    return weighted_majority(h, z)\n",
        "\n",
        "\n",
        "def weighted_majority(predictors, weights):\n",
        "    \"\"\"Return a predictor that takes a weighted vote.\"\"\"\n",
        "\n",
        "    def predict(example):\n",
        "        return weighted_mode((predictor(example) for predictor in predictors), weights)\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def weighted_mode(values, weights):\n",
        "    \"\"\"\n",
        "    Return the value with the greatest total weight.\n",
        "    >>> weighted_mode('abbaa', [1, 2, 3, 1, 2])\n",
        "    'b'\n",
        "    \"\"\"\n",
        "    totals = defaultdict(int)\n",
        "    for v, w in zip(values, weights):\n",
        "        totals[v] += w\n",
        "    return max(totals, key=totals.__getitem__)\n",
        "\n",
        "\n",
        "def RandomForest(dataset, n=5):\n",
        "    \"\"\"An ensemble of Decision Trees trained using bagging and feature bagging.\"\"\"\n",
        "\n",
        "    def data_bagging(dataset, m=0):\n",
        "        \"\"\"Sample m examples with replacement\"\"\"\n",
        "        n = len(dataset.examples)\n",
        "        return weighted_sample_with_replacement(m or n, dataset.examples, [1] * n)\n",
        "\n",
        "    def feature_bagging(dataset, p=0.7):\n",
        "        \"\"\"Feature bagging with probability p to retain an attribute\"\"\"\n",
        "        inputs = [i for i in dataset.inputs if probability(p)]\n",
        "        return inputs or dataset.inputs\n",
        "\n",
        "    def predict(example):\n",
        "        print([predictor(example) for predictor in predictors])\n",
        "        return mode(predictor(example) for predictor in predictors)\n",
        "\n",
        "    predictors = [DecisionTreeLearner(DataSet(examples=data_bagging(dataset), attrs=dataset.attrs,\n",
        "                                              attr_names=dataset.attr_names, target=dataset.target,\n",
        "                                              inputs=feature_bagging(dataset))) for _ in range(n)]\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def WeightedLearner(unweighted_learner):\n",
        "    \"\"\"\n",
        "    [Page 749 footnote 14]\n",
        "    Given a learner that takes just an unweighted dataset, return\n",
        "    one that takes also a weight for each example.\n",
        "    \"\"\"\n",
        "\n",
        "    def train(dataset, weights):\n",
        "        return unweighted_learner(replicated_dataset(dataset, weights))\n",
        "\n",
        "    return train\n",
        "\n",
        "\n",
        "def replicated_dataset(dataset, weights, n=None):\n",
        "    \"\"\"Copy dataset, replicating each example in proportion to its weight.\"\"\"\n",
        "    n = n or len(dataset.examples)\n",
        "    result = copy.copy(dataset)\n",
        "    result.examples = weighted_replicate(dataset.examples, weights, n)\n",
        "    return result\n",
        "\n",
        "\n",
        "def weighted_replicate(seq, weights, n):\n",
        "    \"\"\"\n",
        "    Return n selections from seq, with the count of each element of\n",
        "    seq proportional to the corresponding weight (filling in fractions\n",
        "    randomly).\n",
        "    >>> weighted_replicate('ABC', [1, 2, 1], 4)\n",
        "    ['A', 'B', 'B', 'C']\n",
        "    \"\"\"\n",
        "    assert len(seq) == len(weights)\n",
        "    weights = normalize(weights)\n",
        "    wholes = [int(w * n) for w in weights]\n",
        "    fractions = [(w * n) % 1 for w in weights]\n",
        "    return (flatten([x] * nx for x, nx in zip(seq, wholes)) +\n",
        "            weighted_sample_with_replacement(n - sum(wholes), seq, fractions))\n",
        "\n",
        "\n",
        "# metrics\n",
        "\n",
        "def accuracy_score(y_pred, y_true):\n",
        "    assert y_pred.shape == y_true.shape\n",
        "    return np.mean(np.equal(y_pred, y_true))\n",
        "\n",
        "\n",
        "def r2_score(y_pred, y_true):\n",
        "    assert y_pred.shape == y_true.shape\n",
        "    return 1. - (np.sum(np.square(y_pred - y_true)) /  # sum of square of residuals\n",
        "                 np.sum(np.square(y_true - np.mean(y_true))))  # total sum of squares\n",
        "\n",
        "\n",
        "# datasets\n",
        "\n",
        "orings = DataSet(name='orings', target='Distressed', attr_names='Rings Distressed Temp Pressure Flightnum')\n",
        "\n",
        "zoo = DataSet(name='zoo', target='type', exclude=['name'],\n",
        "              attr_names='name hair feathers eggs milk airborne aquatic predator toothed backbone '\n",
        "                         'breathes venomous fins legs tail domestic catsize type')\n",
        "\n",
        "iris = DataSet(name='iris', target='class', attr_names='sepal-len sepal-width petal-len petal-width class')\n",
        "\n",
        "\n",
        "def RestaurantDataSet(examples=None):\n",
        "    \"\"\"\n",
        "    [Figure 18.3]\n",
        "    Build a DataSet of Restaurant waiting examples.\n",
        "    \"\"\"\n",
        "    return DataSet(name='restaurant', target='Wait', examples=examples,\n",
        "                   attr_names='Alternate Bar Fri/Sat Hungry Patrons Price Raining Reservation Type WaitEstimate Wait')\n",
        "\n",
        "\n",
        "restaurant = RestaurantDataSet()\n",
        "\n",
        "\n",
        "def T(attr_name, branches):\n",
        "    branches = {value: (child if isinstance(child, DecisionFork) else DecisionLeaf(child))\n",
        "                for value, child in branches.items()}\n",
        "    return DecisionFork(restaurant.attr_num(attr_name), attr_name, print, branches)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "[Figure 18.2]\n",
        "A decision tree for deciding whether to wait for a table at a hotel.\n",
        "\"\"\"\n",
        "\n",
        "waiting_decision_tree = T('Patrons',\n",
        "                          {'None': 'No', 'Some': 'Yes',\n",
        "                           'Full': T('WaitEstimate',\n",
        "                                     {'>60': 'No', '0-10': 'Yes',\n",
        "                                      '30-60': T('Alternate',\n",
        "                                                 {'No': T('Reservation',\n",
        "                                                          {'Yes': 'Yes',\n",
        "                                                           'No': T('Bar', {'No': 'No',\n",
        "                                                                           'Yes': 'Yes'})}),\n",
        "                                                  'Yes': T('Fri/Sat', {'No': 'No', 'Yes': 'Yes'})}),\n",
        "                                      '10-30': T('Hungry',\n",
        "                                                 {'No': 'Yes',\n",
        "                                                  'Yes': T('Alternate',\n",
        "                                                           {'No': 'Yes',\n",
        "                                                            'Yes': T('Raining',\n",
        "                                                                     {'No': 'No',\n",
        "                                                                      'Yes': 'Yes'})})})})})\n",
        "\n",
        "\n",
        "def SyntheticRestaurant(n=20):\n",
        "    \"\"\"Generate a DataSet with n examples.\"\"\"\n",
        "\n",
        "    def gen():\n",
        "        example = list(map(random.choice, restaurant.values))\n",
        "        example[restaurant.target] = waiting_decision_tree(example)\n",
        "        return example\n",
        "\n",
        "    return RestaurantDataSet([gen() for _ in range(n)])\n",
        "\n",
        "\n",
        "def Majority(k, n):\n",
        "    \"\"\"\n",
        "    Return a DataSet with n k-bit examples of the majority problem:\n",
        "    k random bits followed by a 1 if more than half the bits are 1, else 0.\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for i in range(n):\n",
        "        bits = [random.choice([0, 1]) for _ in range(k)]\n",
        "        bits.append(int(sum(bits) > k / 2))\n",
        "        examples.append(bits)\n",
        "    return DataSet(name='majority', examples=examples)\n",
        "\n",
        "\n",
        "def Parity(k, n, name='parity'):\n",
        "    \"\"\"\n",
        "    Return a DataSet with n k-bit examples of the parity problem:\n",
        "    k random bits followed by a 1 if an odd number of bits are 1, else 0.\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for i in range(n):\n",
        "        bits = [random.choice([0, 1]) for _ in range(k)]\n",
        "        bits.append(sum(bits) % 2)\n",
        "        examples.append(bits)\n",
        "    return DataSet(name=name, examples=examples)\n",
        "\n",
        "\n",
        "def Xor(n):\n",
        "    \"\"\"Return a DataSet with n examples of 2-input xor.\"\"\"\n",
        "    return Parity(2, n, name='xor')\n",
        "\n",
        "\n",
        "def ContinuousXor(n):\n",
        "    \"\"\"2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints.\"\"\"\n",
        "    examples = []\n",
        "    for i in range(n):\n",
        "        x, y = [random.uniform(0.0, 2.0) for _ in '12']\n",
        "        examples.append([x, y, x != y])\n",
        "    return DataSet(name='continuous xor', examples=examples)\n",
        "\n",
        "\n",
        "def compare(algorithms=None, datasets=None, k=10, trials=1):\n",
        "    \"\"\"\n",
        "    Compare various learners on various datasets using cross-validation.\n",
        "    Print results as a table.\n",
        "    \"\"\"\n",
        "    # default list of algorithms\n",
        "    algorithms = algorithms or [PluralityLearner, NaiveBayesLearner, NearestNeighborLearner, DecisionTreeLearner]\n",
        "\n",
        "    # default list of datasets\n",
        "    datasets = datasets or [iris, orings, zoo, restaurant, SyntheticRestaurant(20),\n",
        "                            Majority(7, 100), Parity(7, 100), Xor(100)]\n",
        "\n",
        "    print_table([[a.__name__.replace('Learner', '')] + [cross_validation(a, d, k=k, trials=trials) for d in datasets]\n",
        "                 for a in algorithms], header=[''] + [d.name[0:7] for d in datasets], numfmt='%.2f')"
      ]
    }
  ]
}